---
title: "Untitled"
author: "RN7"
date: "9/21/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In analytics for any particular field it's not enough to be able to create output (fancy charts, dashboards, reports, etc.) but also be able to collect data in a easy, reproducible, but most importantly consistent way. This is all the more important in a field like sports where throughout the course of a season, new data is being updated to a database or some kind of folder. In this blog post I will go over how to create an ETL pipeline for Canada Premier League data hosted in a Google Drive folder (courtesy of Centre Circle & StatsPerform) using R and Github Actions. 

The simple example I'll go over will guide you on how to set up a Google service account and create a Github Actions workflow that runs a few R scripts. The end-product is a very simple ggplot2 chart using the data you downloaded using this workflow.

For more analytical and visualization based blog posts have a look at other blog posts on my website or check out my [soccer_ggplots]() Github repository.

Let's get started!

# CanPL data


a data initiative started by 



StatsPerform and CentreCircle



# Instructions


## Google Drive 

1. Go to "Google Cloud Platform". Make sure you're already signed into your google account.

![]()

2. Go to "IAM & Admin" section on the left-side menu bar.

![]()

3. Create a new project. Give it a good name that states the purpose of your project.

![]()

4. Create service account. Fill in service account details. 

![]()

5. Select Role "Owner" or other as is relevant for your project.

![]()

6. Once done click on your service account from the project page.

![]()

7. Go to "Keys" tab and click on "ADD KEY" and then "Create new key".

![]()

8. Make sure the key type is "JSON" and create it.

![]()

9. Store the file in a secure space (make sure it's NOT being uploaded into a public repository on Github by .gitignore-ing the file or making it an environment variable)

![]()

10. Go to your Google Drive API page and enable the API. URL link is: https://console.developers.google.com/apis/api/drive.googleapis.com/overview?project={YOUR-PROJECT-ID}.

![]()

11. Have owner of folder/file share it with the service account. NOTE: Since the service account doesn't have a physical email inbox, you can't send an email with the link to it and open it from the email message. You have to make sure to share the folder/file from Google Drive directly.

**NOTE**: Please only do this part if you are serious about using the Canada Premier League data. Otherwise you can create your own separate folder on Google Drive, put some data in it, and share that to your Google service account and run the Github Actions workflow on that instead.

![]()

## Github Actions

[Github Actions](https://github.com/features/actions) is a relatively new feature introduced in late 2019 that allows you to set up workflows and take advantage of Github's VMs (Windows, Mac, Linux, or your own setup).



[Github Actions with R book](https://orchid00.github.io/actions_sandbox/) and/or look up other people's GHA yaml files online

R has considerable support for using Github Actions to power your analyses. Take a look at R-Lib's GHA repository which will give you templates and commands to set up different workflows for your needs.



Some other examples of using R and GHA:

* [Automate Web Scraping in R with Github Actions](https://www.youtube.com/watch?v=N3NrWMxeeJQ)
* [Automating web scraping with GitHub Actions and R: an example from New Jersey](https://www.gavinrozzi.com/post/automating-scraping-gh-actions/)
* [R-Package GitHub Actions via {usethis} and r-lib](https://www.rostrum.blog/2020/08/09/ghactions-pkgs/)
* [Up-to-date blog stats in your README](https://www.rostrum.blog/2021/04/14/gha-readme/)
* [Launch an R script using github actions (R for SEO book)](https://www.rforseo.com/ressources/launch-an-r-script-using-github-actions)
* [A Twitter bot with {rtweet} and GitHub Actions](https://www.rostrum.blog/2020/09/21/londonmapbot/)

- Have a github repository setup with the scripts and other materials you want to use.
- Open the repo up in RStudio and type in: `usethis::use_github_actions()`. This will do all the set up for you to get GHA running on your repository.
- Your GHA workflows are stored in `.github/workflows` as YAML files. If you used the function above it'll create one for R-CMD-check for you. You don't need to for what we're doing since this repository isn't a package. Either delete it or modify it for what we want to do.
- Note that for both private and public repositories you have a number of **free credits to use per month** but anything more is going to cost you.


To let Github Actions workflow use your Google credentials, you need to store it in a place where GHA can retrieve it.

1. Go into "Settings" in your Github repository.

2. Click on "Secrets".

3. Click on "New repository secret".

4. Call it whatever you want then copy-paste the contents of the `.JSON` file into the "value" prompt.




### Workflow YAML file

Within the `.github/workflows/` directory 


__Basic steps__ are as follows:

1. It does some set up with installing R (`r-lib/actions/setup-r@v1`) and git check out (`actions/checkout@v2`).

2. If you're using Ubuntu-Linux as the VM running this workflow, then you need to install libcurl openssl to be able to install the googledrive package in later steps (mainly due to curl and httr dependency R packages, see this [StackOverflow post](https://stackoverflow.com/questions/20923209/problems-installing-the-devtools-package        
) for details). You don't need to do this if you're using MacOS as the VM.

3. Installs R packages from CRAN. Be warned that due to some dependencies your package might not actually install even if GHA says that step was completed.

4. Runs the R script `Get-GoogleDrive-Data.R`.

5. Runs the R script `Plot-ggplot.R`.

6. Commits and pushes the data files downloaded into the Github repository.

Make sure your __indentations__ for each section are correct or it won't run properly. It can be a pain in the ass but it is what it is. I usually just copy-paste someone else's YAML file that has all the steps and setups close to what I want to do and then just start editing from there.

More details:

`on`: "When" to run this action. Can be on git push, pull-request, etc. (use `[]` when specifying multiple conditions) or you can schedule it using cron. 

`runs-on`: Which OS do you want to run this GHA on? Note that per the terms of [GHA minutes and billing](https://docs.github.com/en/billing/managing-billing-for-github-actions/about-billing-for-github-actions), using Ubuntu-Linux is the cheapest, then its Windows, and then Mac is the most expensive so plan accordingly.

`env`: This is where you refer to the environment variables you have set up in your Github repository. Stuff like your Github Token and your Google credentials. 

`steps`: This is where you outline the specific steps your workflow should take. 


Now... how can I refer to my Google credentials stored as a Github secret in the `googledrive::drive_auth()` function?

Because you specified the Github secret as "GOOGLE_AUTHENTICATION_CREDENTIALS", this is the name you need to refer to when using `Sys.getenv()` to grab that environment variable.



Remember that you need to "git pull" your repo whenever so that you have the latest data to work with

This is because when it downloads and commits the data into your github repo, it is only updating it online on github and not on your local computer. So you need to pull all the new stuff in first or you'll be working with unupdated data from the last time you pulled.



## R Scripts

For both of these scripts I tried using the minimal amount of packages to reduce dependencies that I would have to download and install as part of the GHA workflow. You could easily use a `for` loop instead of `purrr::map2()` and base R plotting instead of {ggplot2} if you want to go even further (I attempted this in `scripts/Plot-BaseR.R` but it was just quicker doing it with ggplot2).

### `Get-GoogleDrive-Data.R`

1. Load R packages.

2. Authenticate Google Drive by fetching the environment variable you set up in the Github repository as a Github secret.

3. Find the Google Drive folder you want to grab data from.

4. Filter the folder for the `.csv` files.

5. Create a download function that grabs the `.csv` files and saves them in the `data/` folder. Add some handy messages throughout the function so that it will show up in the GHA log (this helps with debugging and just knowing what's going on as the workflow runs).

6. Now use `purrr::map2()` to iterate the download function to each individual `.csv` file in the folder.

![]()


### `Plot-ggplot.R`

1. Load R packages.

2. Read data from the `data/` folder.

3. Do some data cleaning and create some non-penalty version of the variables.

4. Create a very basic bar chart.

5. Save it in the `basic_plots/` folder.

![]()

# Conclusion



* Create a Twitter bot to post your visualizations
* Create a parameterized RMarkdown report
* Create a Rmarkdown dashboard
* Use the updated data to power a Shiny app
* Create your own separate database and upload new data into it after some cleaning steps
* Etc.




